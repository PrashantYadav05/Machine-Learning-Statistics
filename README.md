# Statistics for Machine Learning:
## Regression Statistics:
* Simple regression
* Multilinear regression: Backward selection, iterations start with considering all the variables & removed one by one until all the presciebed statistics are met such as no significance & multi-collinearity. Finally overall ststistics will be checked, such as R-squared value < 0.7, considered as good model, else reject it.
* Ridge & Lasso: A penalty is applied (shrinkage penalty) on coefficient value to regualrize the coefficient with tuning parameter lambda. When lamba is 0, the penalty has no impact
## Logistic Regression Versus Random Forest:

## Tree-Based Machine Learning Models:
Dataset description: Whether employees would attrite or not based on independent explanatory variables.
* Decision trees - simple model and model with class weight tuning
* Bagging (bootstrap aggregation)
* Random Forest - basic random forest and application of grid search on hypyerparameter tuning
* Boosting (AdaBoost, gradient boost, extreme gradient boost - XGBoost)
* Ensemble of ensembles (with heterogeneous and homogeneous models)

## K-Nearest Neighbors and Naive Bayes:

## Support Vector Machines:
Useful in high dimension data.
**Dataset decription:**  Letter recognition data. The task is to identify each of large number of black & white rectangular pixel displays as one of the 26 capital letters in English Alphabets based on few charactersistics in integers, such as xbox (horizontal position of box), ybox (vertical position of box), width of box, height of box and so on.

## Recommendation Engines

## Unsupervised Learning
